{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Direct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jeremie/anaconda3/envs/pytorch_test/lib/python3.7/site-packages/torch/nn/modules/rnn.py:38: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n",
      "Starting initializing clwe\n",
      "324 matches out of 21767 words\n",
      "Finished initializing clwe\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   1 | time: 27.90s | valid loss  7.25 | valid ppl  1404.04\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   2 | time: 27.60s | valid loss  7.07 | valid ppl  1171.45\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   3 | time: 25.04s | valid loss  6.95 | valid ppl  1043.02\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   4 | time: 26.83s | valid loss  6.83 | valid ppl   929.61\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   5 | time: 25.14s | valid loss  6.77 | valid ppl   875.53\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   6 | time: 25.78s | valid loss  6.72 | valid ppl   831.73\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   7 | time: 25.67s | valid loss  6.65 | valid ppl   771.25\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   8 | time: 25.70s | valid loss  6.62 | valid ppl   747.58\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   9 | time: 26.09s | valid loss  6.59 | valid ppl   724.43\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  10 | time: 26.24s | valid loss  6.54 | valid ppl   690.31\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  11 | time: 25.81s | valid loss  6.52 | valid ppl   681.49\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  12 | time: 26.50s | valid loss  6.50 | valid ppl   667.80\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  13 | time: 25.57s | valid loss  6.49 | valid ppl   657.99\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  14 | time: 27.05s | valid loss  6.48 | valid ppl   651.31\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  15 | time: 26.30s | valid loss  6.47 | valid ppl   646.51\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  16 | time: 26.92s | valid loss  6.45 | valid ppl   631.49\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  17 | time: 27.00s | valid loss  6.45 | valid ppl   634.14\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  18 | time: 25.08s | valid loss  6.43 | valid ppl   621.83\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  19 | time: 26.42s | valid loss  6.43 | valid ppl   621.59\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  20 | time: 26.40s | valid loss  6.43 | valid ppl   621.40\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  21 | time: 26.73s | valid loss  6.43 | valid ppl   619.26\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  22 | time: 26.58s | valid loss  6.43 | valid ppl   620.09\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  23 | time: 25.48s | valid loss  6.43 | valid ppl   620.02\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  24 | time: 24.02s | valid loss  6.43 | valid ppl   620.20\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  25 | time: 25.42s | valid loss  6.43 | valid ppl   620.20\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  26 | time: 25.56s | valid loss  6.43 | valid ppl   620.18\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  27 | time: 25.03s | valid loss  6.43 | valid ppl   620.18\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  28 | time: 25.78s | valid loss  6.43 | valid ppl   620.18\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  29 | time: 26.54s | valid loss  6.43 | valid ppl   620.18\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  30 | time: 26.68s | valid loss  6.43 | valid ppl   620.18\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  31 | time: 25.78s | valid loss  6.43 | valid ppl   620.18\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  32 | time: 26.01s | valid loss  6.43 | valid ppl   620.18\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  33 | time: 27.10s | valid loss  6.43 | valid ppl   620.18\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  34 | time: 27.01s | valid loss  6.43 | valid ppl   620.18\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  35 | time: 25.30s | valid loss  6.43 | valid ppl   620.18\n",
      "-----------------------------------------------------------------------------------------\n",
      "=========================================================================================\n",
      "| End of training | test loss  6.50 | test ppl   668.35\n",
      "=========================================================================================\n"
     ]
    }
   ],
   "source": [
    "!command python ../../src/main.py \\\n",
    "    --data ../../data/transformed/micmac \\\n",
    "    --model GRU \\\n",
    "    --epochs=35 \\\n",
    "    --log-interval 25 \\\n",
    "    --tied \\\n",
    "    --nlayers 1 \\\n",
    "    --lr 5 \\\n",
    "    --dropout 0.5 \\\n",
    "    --hin_weights normal \\\n",
    "    --hr_weights normal \\\n",
    "    --em_weights normal \\\n",
    "    --out_weights normal \\\n",
    "    --emsize 300 \\\n",
    "    --nhid 300 \\\n",
    "    --use_clwe \\\n",
    "    --clwe_method SIMPLE \\\n",
    "    --panlex_loc ../../data/lexicon/eng-mic3.txt \\\n",
    "    --clwe_save ../../models/embeddings/wiki.en \\\n",
    "    --save ../../models/rnns/init_models/gru_300_clwe.pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jeremie/anaconda3/envs/pytorch_test/lib/python3.7/site-packages/torch/nn/modules/rnn.py:38: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n",
      "Starting initializing clwe\n",
      "324 matches out of 21767 words\n",
      "Finished initializing clwe\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   1 | time: 27.38s | valid loss  7.19 | valid ppl  1324.82\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   2 | time: 27.25s | valid loss  7.05 | valid ppl  1157.82\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   3 | time: 26.76s | valid loss  6.98 | valid ppl  1069.98\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   4 | time: 26.45s | valid loss  6.89 | valid ppl   986.59\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   5 | time: 26.67s | valid loss  6.84 | valid ppl   933.43\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   6 | time: 27.80s | valid loss  6.78 | valid ppl   881.67\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   7 | time: 27.32s | valid loss  6.73 | valid ppl   838.35\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   8 | time: 25.69s | valid loss  6.68 | valid ppl   800.10\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   9 | time: 27.70s | valid loss  6.66 | valid ppl   779.58\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  10 | time: 27.28s | valid loss  6.63 | valid ppl   754.72\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  11 | time: 27.49s | valid loss  6.60 | valid ppl   737.94\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  12 | time: 27.48s | valid loss  6.57 | valid ppl   715.26\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  13 | time: 26.59s | valid loss  6.57 | valid ppl   709.93\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  14 | time: 27.40s | valid loss  6.54 | valid ppl   693.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  15 | time: 26.80s | valid loss  6.53 | valid ppl   687.81\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  16 | time: 26.68s | valid loss  6.54 | valid ppl   695.02\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  17 | time: 26.76s | valid loss  6.49 | valid ppl   661.68\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  18 | time: 27.55s | valid loss  6.49 | valid ppl   660.46\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  19 | time: 25.62s | valid loss  6.49 | valid ppl   661.11\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  20 | time: 26.69s | valid loss  6.49 | valid ppl   658.62\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  21 | time: 26.35s | valid loss  6.49 | valid ppl   658.75\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  22 | time: 27.81s | valid loss  6.49 | valid ppl   658.59\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  23 | time: 27.45s | valid loss  6.49 | valid ppl   658.48\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  24 | time: 26.91s | valid loss  6.49 | valid ppl   658.07\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  25 | time: 26.83s | valid loss  6.49 | valid ppl   658.19\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  26 | time: 27.61s | valid loss  6.49 | valid ppl   658.25\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  27 | time: 27.31s | valid loss  6.49 | valid ppl   658.26\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  28 | time: 26.67s | valid loss  6.49 | valid ppl   658.28\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  29 | time: 27.48s | valid loss  6.49 | valid ppl   658.28\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  30 | time: 27.59s | valid loss  6.49 | valid ppl   658.28\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  31 | time: 27.99s | valid loss  6.49 | valid ppl   658.28\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  32 | time: 27.32s | valid loss  6.49 | valid ppl   658.28\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  33 | time: 25.38s | valid loss  6.49 | valid ppl   658.28\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  34 | time: 26.44s | valid loss  6.49 | valid ppl   658.28\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  35 | time: 26.54s | valid loss  6.49 | valid ppl   658.28\n",
      "-----------------------------------------------------------------------------------------\n",
      "=========================================================================================\n",
      "| End of training | test loss  6.56 | test ppl   708.09\n",
      "=========================================================================================\n"
     ]
    }
   ],
   "source": [
    "!command python ../../src/main.py \\\n",
    "    --data ../../data/transformed/micmac \\\n",
    "    --model LSTM \\\n",
    "    --epochs=35 \\\n",
    "    --log-interval 25 \\\n",
    "    --tied \\\n",
    "    --nlayers 1 \\\n",
    "    --lr 5 \\\n",
    "    --dropout 0.5 \\\n",
    "    --hin_weights normal \\\n",
    "    --hr_weights normal \\\n",
    "    --em_weights normal \\\n",
    "    --out_weights normal \\\n",
    "    --emsize 300 \\\n",
    "    --nhid 300 \\\n",
    "    --use_clwe \\\n",
    "    --clwe_method SIMPLE \\\n",
    "    --panlex_loc ../../data/lexicon/eng-mic3.txt \\\n",
    "    --clwe_save ../../models/embeddings/wiki.en \\\n",
    "    --save ../../models/rnns/init_models/LSTM_300_clwe.pt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Direct random 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jeremie/anaconda3/envs/pytorch_test/lib/python3.7/site-packages/torch/nn/modules/rnn.py:38: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n",
      "Starting initializing clwe\n",
      "Traceback (most recent call last):\n",
      "  File \"../../src/main.py\", line 201, in <module>\n",
      "    args.out_weights\n",
      "  File \"/home/jeremie/dev/HonoursCode/src/model.py\", line 87, in __init__\n",
      "    self.init_clwe_rand(cl_embeddings, vocab, panlex, concat=clwe_concat)\n",
      "  File \"/home/jeremie/dev/HonoursCode/src/model.py\", line 236, in init_clwe_rand\n",
      "    lexicon_list = list(lexicon.items())\n",
      "AttributeError: 'Panlex' object has no attribute 'items'\n"
     ]
    }
   ],
   "source": [
    "!command python ../../src/main.py \\\n",
    "    --data ../../data/transformed/micmac \\\n",
    "    --model GRU \\\n",
    "    --epochs=35 \\\n",
    "    --log-interval 25 \\\n",
    "    --tied \\\n",
    "    --nlayers 1 \\\n",
    "    --lr 5 \\\n",
    "    --dropout 0.5 \\\n",
    "    --hin_weights normal \\\n",
    "    --hr_weights normal \\\n",
    "    --em_weights normal \\\n",
    "    --out_weights normal \\\n",
    "    --emsize 300 \\\n",
    "    --nhid 300 \\\n",
    "    --use_clwe \\\n",
    "    --clwe_method RAND \\\n",
    "    --panlex_loc ../../data/lexicon/eng-mic3.txt \\\n",
    "    --clwe_save ../../models/embeddings/wiki.en \\\n",
    "    --save ../../models/rnns/init_models/gru_300_clwe_RAND.pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jeremie/anaconda3/envs/pytorch_test/lib/python3.7/site-packages/torch/nn/modules/rnn.py:38: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n",
      "Starting initializing clwe\n",
      "Traceback (most recent call last):\n",
      "  File \"../../src/main.py\", line 201, in <module>\n",
      "    args.out_weights\n",
      "  File \"/home/jeremie/dev/HonoursCode/src/model.py\", line 87, in __init__\n",
      "    self.init_clwe_rand(cl_embeddings, vocab, panlex, concat=clwe_concat)\n",
      "  File \"/home/jeremie/dev/HonoursCode/src/model.py\", line 236, in init_clwe_rand\n",
      "    lexicon_list = list(lexicon.items())\n",
      "AttributeError: 'Panlex' object has no attribute 'items'\n"
     ]
    }
   ],
   "source": [
    "!command python ../../src/main.py \\\n",
    "    --data ../../data/transformed/micmac \\\n",
    "    --model LSTM \\\n",
    "    --epochs=35 \\\n",
    "    --log-interval 25 \\\n",
    "    --tied \\\n",
    "    --nlayers 1 \\\n",
    "    --lr 5 \\\n",
    "    --dropout 0.5 \\\n",
    "    --hin_weights normal \\\n",
    "    --hr_weights normal \\\n",
    "    --em_weights normal \\\n",
    "    --out_weights normal \\\n",
    "    --emsize 300 \\\n",
    "    --nhid 300 \\\n",
    "    --use_clwe \\\n",
    "    --clwe_method RAND \\\n",
    "    --panlex_loc ../../data/lexicon/eng-mic3.txt \\\n",
    "    --clwe_save ../../models/embeddings/wiki.en \\\n",
    "    --save ../../models/rnns/init_models/LSTM_300_clwe_RAND.pt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CLWE Random 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jeremie/anaconda3/envs/pytorch_test/lib/python3.7/site-packages/torch/nn/modules/rnn.py:38: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n",
      "Starting initializing clwe\n",
      "Traceback (most recent call last):\n",
      "  File \"../../src/main.py\", line 201, in <module>\n",
      "    args.out_weights\n",
      "  File \"/home/jeremie/dev/HonoursCode/src/model.py\", line 92, in __init__\n",
      "    self.init_clwe_randtrans(cl_embeddings, vocab, panlex, concat=clwe_concat)\n",
      "  File \"/home/jeremie/dev/HonoursCode/src/model.py\", line 221, in init_clwe_randtrans\n",
      "    for english, micmac in lexicon.items():\n",
      "AttributeError: 'Panlex' object has no attribute 'items'\n"
     ]
    }
   ],
   "source": [
    "!command python ../../src/main.py \\\n",
    "    --data ../../data/transformed/micmac \\\n",
    "    --model GRU \\\n",
    "    --epochs=35 \\\n",
    "    --log-interval 25 \\\n",
    "    --tied \\\n",
    "    --nlayers 1 \\\n",
    "    --lr 5 \\\n",
    "    --dropout 0.5 \\\n",
    "    --hin_weights normal \\\n",
    "    --hr_weights normal \\\n",
    "    --em_weights normal \\\n",
    "    --out_weights normal \\\n",
    "    --emsize 300 \\\n",
    "    --nhid 300 \\\n",
    "    --use_clwe \\\n",
    "    --clwe_method RAND_TRANS \\\n",
    "    --panlex_loc ../../data/lexicon/eng-mic3.txt \\\n",
    "    --clwe_save ../../models/embeddings/wiki.en \\\n",
    "    --save ../../models/rnns/init_models/gru_300_clwe_RAND_TRANS.pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jeremie/anaconda3/envs/pytorch_test/lib/python3.7/site-packages/torch/nn/modules/rnn.py:38: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n",
      "Starting initializing clwe\n",
      "Traceback (most recent call last):\n",
      "  File \"../../src/main.py\", line 201, in <module>\n",
      "    args.out_weights\n",
      "  File \"/home/jeremie/dev/HonoursCode/src/model.py\", line 92, in __init__\n",
      "    self.init_clwe_randtrans(cl_embeddings, vocab, panlex, concat=clwe_concat)\n",
      "  File \"/home/jeremie/dev/HonoursCode/src/model.py\", line 221, in init_clwe_randtrans\n",
      "    for english, micmac in lexicon.items():\n",
      "AttributeError: 'Panlex' object has no attribute 'items'\n"
     ]
    }
   ],
   "source": [
    "!command python ../../src/main.py \\\n",
    "    --data ../../data/transformed/micmac \\\n",
    "    --model LSTM \\\n",
    "    --epochs=35 \\\n",
    "    --log-interval 25 \\\n",
    "    --tied \\\n",
    "    --nlayers 1 \\\n",
    "    --lr 5 \\\n",
    "    --dropout 0.5 \\\n",
    "    --hin_weights normal \\\n",
    "    --hr_weights normal \\\n",
    "    --em_weights normal \\\n",
    "    --out_weights normal \\\n",
    "    --emsize 300 \\\n",
    "    --nhid 300 \\\n",
    "    --use_clwe \\\n",
    "    --clwe_method RAND_TRANS \\\n",
    "    --panlex_loc ../../data/lexicon/eng-mic3.txt \\\n",
    "    --clwe_save ../../models/embeddings/wiki.en \\\n",
    "    --save ../../models/rnns/init_models/LSTM_300_clwe_RAND_TRANS.pt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CLWE Random 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"../../src/main.py\", line 128, in <module>\n",
      "    panlex = data.Panlex(args.panlex_loc, acceptable_dist=args.panlex_dist)\n",
      "  File \"/home/jeremie/dev/HonoursCode/src/utils/data.py\", line 63, in __init__\n",
      "    self.lexicon, self.inverted_lexicon = self.read_lexicon(lexicon_location)\n",
      "  File \"/home/jeremie/dev/HonoursCode/src/utils/data.py\", line 98, in read_lexicon\n",
      "    if words.group(1) != '<UNK>' and words.group(2) != '<UNK>':\n",
      "AttributeError: 'NoneType' object has no attribute 'group'\n"
     ]
    }
   ],
   "source": [
    "!command python ../../src/main.py \\\n",
    "    --data ../../data/transformed/micmac \\\n",
    "    --model GRU \\\n",
    "    --epochs=35 \\\n",
    "    --log-interval 25 \\\n",
    "    --tied \\\n",
    "    --nlayers 1 \\\n",
    "    --lr 5 \\\n",
    "    --dropout 0.5 \\\n",
    "    --hin_weights normal \\\n",
    "    --hr_weights normal \\\n",
    "    --em_weights normal \\\n",
    "    --out_weights normal \\\n",
    "    --emsize 300 \\\n",
    "    --nhid 300 \\\n",
    "    --use_clwe \\\n",
    "    --clwe_method RAND_COMBO \\\n",
    "    --panlex_loc ../../data/lexicon/eng-mic2.txt \\\n",
    "    --clwe_save ../../models/embeddings/wiki.en \\\n",
    "    --save ../../models/rnns/init_models/gru_300_clwe_RAND_COMBO.pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jeremie/anaconda3/envs/pytorch_test/lib/python3.7/site-packages/torch/nn/modules/rnn.py:38: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n",
      "Starting initializing clwe\n",
      "Traceback (most recent call last):\n",
      "  File \"../../src/main.py\", line 201, in <module>\n",
      "    args.out_weights\n",
      "  File \"/home/jeremie/dev/HonoursCode/src/model.py\", line 94, in __init__\n",
      "    self.init_clwe_rand(cl_embeddings, vocab, panlex, concat=clwe_concat)\n",
      "  File \"/home/jeremie/dev/HonoursCode/src/model.py\", line 236, in init_clwe_rand\n",
      "    lexicon_list = list(lexicon.items())\n",
      "AttributeError: 'Panlex' object has no attribute 'items'\n"
     ]
    }
   ],
   "source": [
    "!command python ../../src/main.py \\\n",
    "    --data ../../data/transformed/micmac \\\n",
    "    --model LSTM \\\n",
    "    --epochs=35 \\\n",
    "    --log-interval 25 \\\n",
    "    --tied \\\n",
    "    --nlayers 1 \\\n",
    "    --lr 5 \\\n",
    "    --dropout 0.5 \\\n",
    "    --hin_weights normal \\\n",
    "    --hr_weights normal \\\n",
    "    --em_weights normal \\\n",
    "    --out_weights normal \\\n",
    "    --emsize 300 \\\n",
    "    --nhid 300 \\\n",
    "    --use_clwe \\\n",
    "    --clwe_method RAND_COMBO \\\n",
    "    --panlex_loc ../../data/lexicon/eng-mic3.txt \\\n",
    "    --clwe_save ../../models/embeddings/wiki.en \\\n",
    "    --save ../../models/rnns/init_models/LSTM_300_clwe_RAND_COMBO.pt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Duong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please specify english or duong word embeddigs using the --fasttext_save parameter\n",
      "Traceback (most recent call last):\n",
      "  File \"../../src/main.py\", line 171, in <module>\n",
      "    embeddings=em_model,\n",
      "NameError: name 'em_model' is not defined\n"
     ]
    }
   ],
   "source": [
    "!command python ../../src/main.py \\\n",
    "    --data ../../data/transformed/micmac \\\n",
    "    --model GRU \\\n",
    "    --epochs=35 \\\n",
    "    --log-interval 25 \\\n",
    "    --tied \\\n",
    "    --nlayers 1 \\\n",
    "    --lr 5 \\\n",
    "    --dropout 0.5 \\\n",
    "    --hin_weights normal \\\n",
    "    --hr_weights normal \\\n",
    "    --em_weights normal \\\n",
    "    --out_weights normal \\\n",
    "    --emsize 300 \\\n",
    "    --nhid 300 \\\n",
    "    --use_clwe \\\n",
    "    --clwe_method DUONG \\\n",
    "    --panlex_loc ../../data/lexicon/eng-mic3.txt \\\n",
    "    --clwe_save ../../models/embeddings/duong2/en5mil.mi.word300.emb \\\n",
    "    --save ../../models/rnns/init_models/gru_300_clwe_DUONG_5m.pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please specify english or duong word embeddigs using the --fasttext_save parameter\n",
      "Traceback (most recent call last):\n",
      "  File \"../../src/main.py\", line 171, in <module>\n",
      "    embeddings=em_model,\n",
      "NameError: name 'em_model' is not defined\n"
     ]
    }
   ],
   "source": [
    "!command python ../../src/main.py \\\n",
    "    --data ../../data/transformed/micmac \\\n",
    "    --model GRU \\\n",
    "    --epochs=35 \\\n",
    "    --log-interval 25 \\\n",
    "    --tied \\\n",
    "    --nlayers 1 \\\n",
    "    --lr 5 \\\n",
    "    --dropout 0.5 \\\n",
    "    --hin_weights normal \\\n",
    "    --hr_weights normal \\\n",
    "    --em_weights normal \\\n",
    "    --out_weights normal \\\n",
    "    --emsize 300 \\\n",
    "    --nhid 300 \\\n",
    "    --use_clwe \\\n",
    "    --clwe_method DUONG \\\n",
    "    --panlex_loc ../../data/lexicon/eng-mic3.txt \\\n",
    "    --clwe_save ../../models/embeddings/duong2/en200l.mi.word300.emb \\\n",
    "    --save ../../models/rnns/init_models/gru_300_clwe_DUONG_200k.pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please specify english or duong word embeddigs using the --fasttext_save parameter\n",
      "Traceback (most recent call last):\n",
      "  File \"../../src/main.py\", line 171, in <module>\n",
      "    embeddings=em_model,\n",
      "NameError: name 'em_model' is not defined\n"
     ]
    }
   ],
   "source": [
    "!command python ../../src/main.py \\\n",
    "    --data ../../data/transformed/micmac \\\n",
    "    --model LSTM \\\n",
    "    --epochs=35 \\\n",
    "    --log-interval 25 \\\n",
    "    --tied \\\n",
    "    --nlayers 1 \\\n",
    "    --lr 5 \\\n",
    "    --dropout 0.5 \\\n",
    "    --hin_weights normal \\\n",
    "    --hr_weights normal \\\n",
    "    --em_weights normal \\\n",
    "    --out_weights normal \\\n",
    "    --emsize 300 \\\n",
    "    --nhid 300 \\\n",
    "    --use_clwe \\\n",
    "    --clwe_method DUONG \\\n",
    "    --panlex_loc ../../data/lexicon/eng-mic3.txt \\\n",
    "    --clwe_save ../../models/embeddings/duong2/en5mil.mi.word300.emb \\\n",
    "    --save ../../models/rnns/init_models/LSTM_300_clwe_DUONG_5m.pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please specify english or duong word embeddigs using the --fasttext_save parameter\n",
      "Traceback (most recent call last):\n",
      "  File \"../../src/main.py\", line 171, in <module>\n",
      "    embeddings=em_model,\n",
      "NameError: name 'em_model' is not defined\n"
     ]
    }
   ],
   "source": [
    "!command python ../../src/main.py \\\n",
    "    --data ../../data/transformed/micmac \\\n",
    "    --model LSTM \\\n",
    "    --epochs=35 \\\n",
    "    --log-interval 25 \\\n",
    "    --tied \\\n",
    "    --nlayers 1 \\\n",
    "    --lr 5 \\\n",
    "    --dropout 0.5 \\\n",
    "    --hin_weights normal \\\n",
    "    --hr_weights normal \\\n",
    "    --em_weights normal \\\n",
    "    --out_weights normal \\\n",
    "    --emsize 300 \\\n",
    "    --nhid 300 \\\n",
    "    --use_clwe \\\n",
    "    --clwe_method DUONG \\\n",
    "    --panlex_loc ../../data/lexicon/eng-mic3.txt \\\n",
    "    --clwe_save ../../models/embeddings/duong2/en200k.mi.word300.emb \\\n",
    "    --save ../../models/rnns/init_models/LSTM_300_clwe_DUONG_200k.pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please specify english or duong word embeddigs using the --fasttext_save parameter\n",
      "Traceback (most recent call last):\n",
      "  File \"../../src/main.py\", line 171, in <module>\n",
      "    embeddings=em_model,\n",
      "NameError: name 'em_model' is not defined\n"
     ]
    }
   ],
   "source": [
    "!command python ../../src/main.py \\\n",
    "    --data ../../data/transformed/micmac \\\n",
    "    --model LSTM \\\n",
    "    --epochs=35 \\\n",
    "    --log-interval 25 \\\n",
    "    --tied \\\n",
    "    --nlayers 1 \\\n",
    "    --lr 5 \\\n",
    "    --dropout 0.5 \\\n",
    "    --hin_weights normal \\\n",
    "    --hr_weights normal \\\n",
    "    --em_weights normal \\\n",
    "    --out_weights normal \\\n",
    "    --emsize 300 \\\n",
    "    --nhid 300 \\\n",
    "    --use_clwe \\\n",
    "    --clwe_method DUONG \\\n",
    "    --panlex_loc ../../data/lexicon/eng-mic3.txt \\\n",
    "    --clwe_save ../../models/embeddings/duong2/en5mil.mi.word300.emb \\\n",
    "    --save ../../models/rnns/init_models/LSTM_300_clwe_DUONG_200k.pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please specify english or duong word embeddigs using the --fasttext_save parameter\n",
      "Traceback (most recent call last):\n",
      "  File \"../../src/main.py\", line 171, in <module>\n",
      "    embeddings=em_model,\n",
      "NameError: name 'em_model' is not defined\n"
     ]
    }
   ],
   "source": [
    "!command python ../../src/main.py \\\n",
    "    --data ../../data/transformed/micmac \\\n",
    "    --model GRU \\\n",
    "    --epochs=35 \\\n",
    "    --log-interval 25 \\\n",
    "    --tied \\\n",
    "    --nlayers 1 \\\n",
    "    --lr 5 \\\n",
    "    --dropout 0.5 \\\n",
    "    --hin_weights normal \\\n",
    "    --hr_weights normal \\\n",
    "    --em_weights normal \\\n",
    "    --out_weights normal \\\n",
    "    --emsize 300 \\\n",
    "    --nhid 300 \\\n",
    "    --use_clwe \\\n",
    "    --clwe_method DUONG \\\n",
    "    --panlex_loc ../../data/lexicon/eng-mic3.txt \\\n",
    "    --clwe_save ../../models/embeddings/duong2/en5mil.mi.word200.emb \\\n",
    "    --save ../../models/rnns/init_models/gru_300_clwe_DUONG_200k_f.pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please specify english or duong word embeddigs using the --fasttext_save parameter\n",
      "Traceback (most recent call last):\n",
      "  File \"../../src/main.py\", line 171, in <module>\n",
      "    embeddings=em_model,\n",
      "NameError: name 'em_model' is not defined\n"
     ]
    }
   ],
   "source": [
    "\n",
    "!command python ../../src/main.py \\\n",
    "    --data ../../data/transformed/micmac \\\n",
    "    --model LSTM \\\n",
    "    --epochs=35 \\\n",
    "    --log-interval 25 \\\n",
    "    --tied \\\n",
    "    --nlayers 1 \\\n",
    "    --lr 5 \\\n",
    "    --dropout 0.5 \\\n",
    "    --hin_weights normal \\\n",
    "    --hr_weights normal \\\n",
    "    --em_weights normal \\\n",
    "    --out_weights normal \\\n",
    "    --emsize 300 \\\n",
    "    --nhid 300 \\\n",
    "    --use_clwe \\\n",
    "    --clwe_method DUONG \\\n",
    "    --panlex_loc ../../data/lexicon/eng-mic3.txt \\\n",
    "    --clwe_save ../../models/embeddings/duong2/en5mil.mi.word300.emb \\\n",
    "    --save ../../models/rnns/init_models/lstm_300_clwe_DUONG_200k_f.pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please specify english or duong word embeddigs using the --fasttext_save parameter\n",
      "Traceback (most recent call last):\n",
      "  File \"../../src/main.py\", line 171, in <module>\n",
      "    embeddings=em_model,\n",
      "NameError: name 'em_model' is not defined\n"
     ]
    }
   ],
   "source": [
    "!command python ../../src/main.py \\\n",
    "    --data ../../data/transformed/micmac \\\n",
    "    --model GRU \\\n",
    "    --epochs=35 \\\n",
    "    --log-interval 25 \\\n",
    "    --tied \\\n",
    "    --nlayers 1 \\\n",
    "    --lr 5 \\\n",
    "    --dropout 0.5 \\\n",
    "    --hin_weights normal \\\n",
    "    --hr_weights normal \\\n",
    "    --em_weights normal \\\n",
    "    --out_weights normal \\\n",
    "    --emsize 300 \\\n",
    "    --nhid 300 \\\n",
    "    --use_clwe \\\n",
    "    --clwe_method DUONG \\\n",
    "    --panlex_loc ../../data/lexicon/eng-mic3.txt \\\n",
    "    --clwe_save ../../models/embeddings/duong2/en200k.mi.word300.emb \\\n",
    "    --save ../../models/rnns/init_models/gru_300_clwe_DUONG_200k_fskip.pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please specify english or duong word embeddigs using the --fasttext_save parameter\n",
      "Traceback (most recent call last):\n",
      "  File \"../../src/main.py\", line 171, in <module>\n",
      "    embeddings=em_model,\n",
      "NameError: name 'em_model' is not defined\n"
     ]
    }
   ],
   "source": [
    "!command python ../../src/main.py \\\n",
    "    --data ../../data/transformed/micmac \\\n",
    "    --model LSTM \\\n",
    "    --epochs=35 \\\n",
    "    --log-interval 25 \\\n",
    "    --tied \\\n",
    "    --nlayers 1 \\\n",
    "    --lr 5 \\\n",
    "    --dropout 0.5 \\\n",
    "    --hin_weights normal \\\n",
    "    --hr_weights normal \\\n",
    "    --em_weights normal \\\n",
    "    --out_weights normal \\\n",
    "    --emsize 300 \\\n",
    "    --nhid 300 \\\n",
    "    --use_clwe \\\n",
    "    --clwe_method DUONG \\\n",
    "    --panlex_loc ../../data/lexicon/eng-mic3.txt \\\n",
    "    --clwe_save ../../models/embeddings/duong2/en200k.mi.word300.emb \\\n",
    "    --save ../../models/rnns/init_models/lstm_300_clwe_DUONG_200k_fskip.pt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CONCAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jeremie/anaconda3/envs/pytorch_test/lib/python3.7/site-packages/torch/nn/modules/rnn.py:38: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n",
      "Starting initializing clwe\n",
      "364 matches out of 21767 words\n",
      "Finished initializing clwe\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   1 | time: 51.11s | valid loss  7.24 | valid ppl  1396.35\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   2 | time: 52.56s | valid loss  7.06 | valid ppl  1168.83\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   3 | time: 56.01s | valid loss  6.91 | valid ppl  1004.76\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   4 | time: 52.79s | valid loss  6.80 | valid ppl   894.77\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   5 | time: 53.69s | valid loss  6.73 | valid ppl   834.52\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   6 | time: 52.37s | valid loss  6.67 | valid ppl   789.23\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   7 | time: 49.81s | valid loss  6.61 | valid ppl   739.68\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   8 | time: 49.83s | valid loss  6.57 | valid ppl   714.25\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   9 | time: 49.06s | valid loss  6.54 | valid ppl   695.17\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  10 | time: 53.78s | valid loss  6.54 | valid ppl   690.61\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  11 | time: 57.23s | valid loss  6.51 | valid ppl   669.93\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  12 | time: 54.91s | valid loss  6.48 | valid ppl   654.90\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  13 | time: 51.82s | valid loss  6.47 | valid ppl   647.18\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  14 | time: 52.23s | valid loss  6.46 | valid ppl   641.39\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  15 | time: 54.29s | valid loss  6.47 | valid ppl   642.82\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  16 | time: 55.36s | valid loss  6.44 | valid ppl   626.82\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  17 | time: 53.19s | valid loss  6.44 | valid ppl   626.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  18 | time: 53.13s | valid loss  6.44 | valid ppl   627.55\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  19 | time: 53.96s | valid loss  6.44 | valid ppl   626.58\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  20 | time: 51.99s | valid loss  6.44 | valid ppl   626.28\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  21 | time: 53.09s | valid loss  6.44 | valid ppl   626.32\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  22 | time: 52.92s | valid loss  6.44 | valid ppl   626.28\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  23 | time: 51.58s | valid loss  6.44 | valid ppl   626.27\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  24 | time: 51.58s | valid loss  6.44 | valid ppl   626.28\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  25 | time: 52.18s | valid loss  6.44 | valid ppl   626.28\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  26 | time: 54.65s | valid loss  6.44 | valid ppl   626.28\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  27 | time: 57.89s | valid loss  6.44 | valid ppl   626.28\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  28 | time: 57.13s | valid loss  6.44 | valid ppl   626.28\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  29 | time: 53.83s | valid loss  6.44 | valid ppl   626.28\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  30 | time: 57.12s | valid loss  6.44 | valid ppl   626.28\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  31 | time: 54.08s | valid loss  6.44 | valid ppl   626.28\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  32 | time: 56.53s | valid loss  6.44 | valid ppl   626.28\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  33 | time: 56.82s | valid loss  6.44 | valid ppl   626.28\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  34 | time: 56.44s | valid loss  6.44 | valid ppl   626.28\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  35 | time: 54.31s | valid loss  6.44 | valid ppl   626.28\n",
      "-----------------------------------------------------------------------------------------\n",
      "=========================================================================================\n",
      "| End of training | test loss  6.52 | test ppl   681.03\n",
      "=========================================================================================\n"
     ]
    }
   ],
   "source": [
    "!command python ../../src/main.py \\\n",
    "    --data ../../data/transformed/micmac \\\n",
    "    --model GRU \\\n",
    "    --epochs=35 \\\n",
    "    --log-interval 25 \\\n",
    "    --tied \\\n",
    "    --nlayers 1 \\\n",
    "    --lr 5 \\\n",
    "    --dropout 0.5 \\\n",
    "    --hin_weights normal \\\n",
    "    --hr_weights normal \\\n",
    "    --em_weights normal \\\n",
    "    --out_weights normal \\\n",
    "    --emsize 300 \\\n",
    "    --nhid 300 \\\n",
    "    --use_clwe \\\n",
    "    --clwe_method SIMPLE \\\n",
    "    --clwe_concat \\\n",
    "    --use_fasttext\\\n",
    "    --embedding_model SKIPGRAM \\\n",
    "    --fasttext_save ../../models/embeddings/micmac2/skip300 \\\n",
    "    --panlex_loc ../../data/lexicon/eng-mic3.txt \\\n",
    "    --clwe_save ../../models/embeddings/wiki.en \\\n",
    "    --save ../../models/rnns/init_models/gru_300_clwe_concat.pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jeremie/anaconda3/envs/pytorch_test/lib/python3.7/site-packages/torch/nn/modules/rnn.py:38: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n",
      "Starting initializing clwe\n",
      "Traceback (most recent call last):\n",
      "  File \"../../src/main.py\", line 201, in <module>\n",
      "    args.out_weights\n",
      "  File \"/home/jeremie/dev/HonoursCode/src/model.py\", line 87, in __init__\n",
      "    self.init_clwe_rand(cl_embeddings, vocab, panlex, concat=clwe_concat)\n",
      "  File \"/home/jeremie/dev/HonoursCode/src/model.py\", line 236, in init_clwe_rand\n",
      "    lexicon_list = list(lexicon.items())\n",
      "AttributeError: 'Panlex' object has no attribute 'items'\n"
     ]
    }
   ],
   "source": [
    "!command python ../../src/main.py \\\n",
    "    --data ../../data/transformed/micmac \\\n",
    "    --model GRU \\\n",
    "    --epochs=35 \\\n",
    "    --log-interval 25 \\\n",
    "    --tied \\\n",
    "    --nlayers 1 \\\n",
    "    --lr 5 \\\n",
    "    --dropout 0.5 \\\n",
    "    --hin_weights normal \\\n",
    "    --hr_weights normal \\\n",
    "    --em_weights normal \\\n",
    "    --out_weights normal \\\n",
    "    --emsize 300 \\\n",
    "    --nhid 300 \\\n",
    "    --use_clwe \\\n",
    "    --clwe_method RAND \\\n",
    "    --clwe_concat \\\n",
    "    --use_fasttext\\\n",
    "    --embedding_model SKIPGRAM \\\n",
    "    --fasttext_save ../../models/embeddings/micmac2/skip300 \\\n",
    "    --panlex_loc ../../data/lexicon/eng-mic3.txt \\\n",
    "    --clwe_save ../../models/embeddings/wiki.en \\\n",
    "    --save ../../models/rnns/init_models/gru_300_clwe_rand_concat.pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jeremie/anaconda3/envs/pytorch_test/lib/python3.7/site-packages/torch/nn/modules/rnn.py:38: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n",
      "Starting initializing clwe\n",
      "Traceback (most recent call last):\n",
      "  File \"../../src/main.py\", line 201, in <module>\n",
      "    args.out_weights\n",
      "  File \"/home/jeremie/dev/HonoursCode/src/model.py\", line 92, in __init__\n",
      "    self.init_clwe_randtrans(cl_embeddings, vocab, panlex, concat=clwe_concat)\n",
      "  File \"/home/jeremie/dev/HonoursCode/src/model.py\", line 221, in init_clwe_randtrans\n",
      "    for english, micmac in lexicon.items():\n",
      "AttributeError: 'Panlex' object has no attribute 'items'\n"
     ]
    }
   ],
   "source": [
    "!command python ../../src/main.py \\\n",
    "    --data ../../data/transformed/micmac \\\n",
    "    --model GRU \\\n",
    "    --epochs=35 \\\n",
    "    --log-interval 25 \\\n",
    "    --tied \\\n",
    "    --nlayers 1 \\\n",
    "    --lr 5 \\\n",
    "    --dropout 0.5 \\\n",
    "    --hin_weights normal \\\n",
    "    --hr_weights normal \\\n",
    "    --em_weights normal \\\n",
    "    --out_weights normal \\\n",
    "    --emsize 300 \\\n",
    "    --nhid 300 \\\n",
    "    --use_clwe \\\n",
    "    --clwe_method RAND_TRANS \\\n",
    "    --clwe_concat \\\n",
    "    --use_fasttext\\\n",
    "    --embedding_model SKIPGRAM \\\n",
    "    --fasttext_save ../../models/embeddings/micmac2/skip300 \\\n",
    "    --panlex_loc ../../data/lexicon/eng-mic3.txt \\\n",
    "    --clwe_save ../../models/embeddings/wiki.en \\\n",
    "    --save ../../models/rnns/init_models/gru_300_clwe_randtrans_concat.pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jeremie/anaconda3/envs/pytorch_test/lib/python3.7/site-packages/torch/nn/modules/rnn.py:38: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n",
      "Starting initializing clwe\n",
      "Traceback (most recent call last):\n",
      "  File \"../../src/main.py\", line 201, in <module>\n",
      "    args.out_weights\n",
      "  File \"/home/jeremie/dev/HonoursCode/src/model.py\", line 94, in __init__\n",
      "    self.init_clwe_rand(cl_embeddings, vocab, panlex, concat=clwe_concat)\n",
      "  File \"/home/jeremie/dev/HonoursCode/src/model.py\", line 236, in init_clwe_rand\n",
      "    lexicon_list = list(lexicon.items())\n",
      "AttributeError: 'Panlex' object has no attribute 'items'\n"
     ]
    }
   ],
   "source": [
    "!command python ../../src/main.py \\\n",
    "    --data ../../data/transformed/micmac \\\n",
    "    --model GRU \\\n",
    "    --epochs=35 \\\n",
    "    --log-interval 25 \\\n",
    "    --tied \\\n",
    "    --nlayers 1 \\\n",
    "    --lr 5 \\\n",
    "    --dropout 0.5 \\\n",
    "    --hin_weights normal \\\n",
    "    --hr_weights normal \\\n",
    "    --em_weights normal \\\n",
    "    --out_weights normal \\\n",
    "    --emsize 300 \\\n",
    "    --nhid 300 \\\n",
    "    --use_clwe \\\n",
    "    --clwe_method RAND_COMBO \\\n",
    "    --clwe_concat \\\n",
    "    --use_fasttext\\\n",
    "    --embedding_model SKIPGRAM \\\n",
    "    --fasttext_save ../../models/embeddings/micmac2/skip300 \\\n",
    "    --panlex_loc ../../data/lexicon/eng-mic3.txt \\\n",
    "    --clwe_save ../../models/embeddings/wiki.en \\\n",
    "    --save ../../models/rnns/init_models/gru_300_clwe_randcombo_concat.pt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Edit = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jeremie/anaconda3/envs/pytorch_test/lib/python3.7/site-packages/torch/nn/modules/rnn.py:38: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n",
      "Starting initializing clwe\n",
      "1268 matches out of 21767 words\n",
      "Finished initializing clwe\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   1 | time: 25.54s | valid loss  7.27 | valid ppl  1442.61\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   2 | time: 26.14s | valid loss  7.06 | valid ppl  1161.73\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   3 | time: 26.00s | valid loss  6.96 | valid ppl  1058.29\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   4 | time: 24.98s | valid loss  6.87 | valid ppl   958.48\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   5 | time: 24.83s | valid loss  6.78 | valid ppl   877.43\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   6 | time: 26.29s | valid loss  6.72 | valid ppl   829.51\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   7 | time: 25.26s | valid loss  6.68 | valid ppl   792.90\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   8 | time: 25.22s | valid loss  6.64 | valid ppl   763.34\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   9 | time: 25.33s | valid loss  6.61 | valid ppl   740.02\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  10 | time: 26.17s | valid loss  6.56 | valid ppl   705.80\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  11 | time: 25.56s | valid loss  6.55 | valid ppl   696.11\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  12 | time: 25.91s | valid loss  6.51 | valid ppl   675.09\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  13 | time: 25.17s | valid loss  6.50 | valid ppl   662.53\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  14 | time: 25.42s | valid loss  6.48 | valid ppl   652.78\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  15 | time: 25.30s | valid loss  6.48 | valid ppl   648.81\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  16 | time: 23.15s | valid loss  6.46 | valid ppl   639.26\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  17 | time: 25.81s | valid loss  6.45 | valid ppl   632.82\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  18 | time: 25.16s | valid loss  6.45 | valid ppl   633.71\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  19 | time: 25.48s | valid loss  6.43 | valid ppl   621.17\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  20 | time: 24.88s | valid loss  6.43 | valid ppl   620.22\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  21 | time: 25.71s | valid loss  6.43 | valid ppl   619.47\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  22 | time: 25.99s | valid loss  6.43 | valid ppl   621.15\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  23 | time: 25.44s | valid loss  6.43 | valid ppl   620.47\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  24 | time: 25.05s | valid loss  6.43 | valid ppl   620.45\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  25 | time: 25.28s | valid loss  6.43 | valid ppl   620.25\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  26 | time: 25.64s | valid loss  6.43 | valid ppl   620.25\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  27 | time: 26.22s | valid loss  6.43 | valid ppl   620.24\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  28 | time: 26.30s | valid loss  6.43 | valid ppl   620.23\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  29 | time: 25.83s | valid loss  6.43 | valid ppl   620.23\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  30 | time: 26.03s | valid loss  6.43 | valid ppl   620.23\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  31 | time: 26.26s | valid loss  6.43 | valid ppl   620.23\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  32 | time: 26.23s | valid loss  6.43 | valid ppl   620.23\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  33 | time: 23.04s | valid loss  6.43 | valid ppl   620.23\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  34 | time: 22.87s | valid loss  6.43 | valid ppl   620.23\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  35 | time: 25.31s | valid loss  6.43 | valid ppl   620.23\n",
      "-----------------------------------------------------------------------------------------\n",
      "=========================================================================================\n",
      "| End of training | test loss  6.52 | test ppl   676.29\n",
      "=========================================================================================\n"
     ]
    }
   ],
   "source": [
    "!command python ../../src/main.py \\\n",
    "    --data ../../data/transformed/micmac \\\n",
    "    --model GRU \\\n",
    "    --epochs=35 \\\n",
    "    --log-interval 25 \\\n",
    "    --tied \\\n",
    "    --nlayers 1 \\\n",
    "    --lr 5 \\\n",
    "    --dropout 0.5 \\\n",
    "    --hin_weights normal \\\n",
    "    --hr_weights normal \\\n",
    "    --em_weights normal \\\n",
    "    --out_weights normal \\\n",
    "    --emsize 300 \\\n",
    "    --nhid 300 \\\n",
    "    --use_clwe \\\n",
    "    --clwe_method SIMPLE \\\n",
    "    --panlex_loc ../../data/lexicon/eng-mic3.txt \\\n",
    "    --panlex_dist 1 \\\n",
    "    --clwe_save ../../models/embeddings/wiki.en \\\n",
    "    --save ../../models/rnns/init_models/gru_300_clwe_dist1.pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jeremie/anaconda3/envs/pytorch_test/lib/python3.7/site-packages/torch/nn/modules/rnn.py:38: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n",
      "Starting initializing clwe\n",
      "Traceback (most recent call last):\n",
      "  File \"../../src/main.py\", line 201, in <module>\n",
      "    args.out_weights\n",
      "  File \"/home/jeremie/dev/HonoursCode/src/model.py\", line 87, in __init__\n",
      "    self.init_clwe_rand(cl_embeddings, vocab, panlex, concat=clwe_concat)\n",
      "  File \"/home/jeremie/dev/HonoursCode/src/model.py\", line 236, in init_clwe_rand\n",
      "    lexicon_list = list(lexicon.items())\n",
      "AttributeError: 'Panlex' object has no attribute 'items'\n"
     ]
    }
   ],
   "source": [
    "!command python ../../src/main.py \\\n",
    "    --data ../../data/transformed/micmac \\\n",
    "    --model GRU \\\n",
    "    --epochs=35 \\\n",
    "    --log-interval 25 \\\n",
    "    --tied \\\n",
    "    --nlayers 1 \\\n",
    "    --lr 5 \\\n",
    "    --dropout 0.5 \\\n",
    "    --hin_weights normal \\\n",
    "    --hr_weights normal \\\n",
    "    --em_weights normal \\\n",
    "    --out_weights normal \\\n",
    "    --emsize 300 \\\n",
    "    --nhid 300 \\\n",
    "    --use_clwe \\\n",
    "    --clwe_method RAND \\\n",
    "    --panlex_loc ../../data/lexicon/eng-mic3.txt \\\n",
    "    --panlex_dist 1 \\\n",
    "    --clwe_save ../../models/embeddings/wiki.en \\\n",
    "    --save ../../models/rnns/init_models/gru_300_clwe_rand_dist1.pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jeremie/anaconda3/envs/pytorch_test/lib/python3.7/site-packages/torch/nn/modules/rnn.py:38: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n",
      "Starting initializing clwe\n",
      "Traceback (most recent call last):\n",
      "  File \"../../src/main.py\", line 201, in <module>\n",
      "    args.out_weights\n",
      "  File \"/home/jeremie/dev/HonoursCode/src/model.py\", line 92, in __init__\n",
      "    self.init_clwe_randtrans(cl_embeddings, vocab, panlex, concat=clwe_concat)\n",
      "  File \"/home/jeremie/dev/HonoursCode/src/model.py\", line 221, in init_clwe_randtrans\n",
      "    for english, micmac in lexicon.items():\n",
      "AttributeError: 'Panlex' object has no attribute 'items'\n"
     ]
    }
   ],
   "source": [
    "!command python ../../src/main.py \\\n",
    "    --data ../../data/transformed/micmac \\\n",
    "    --model GRU \\\n",
    "    --epochs=35 \\\n",
    "    --log-interval 25 \\\n",
    "    --tied \\\n",
    "    --nlayers 1 \\\n",
    "    --lr 5 \\\n",
    "    --dropout 0.5 \\\n",
    "    --hin_weights normal \\\n",
    "    --hr_weights normal \\\n",
    "    --em_weights normal \\\n",
    "    --out_weights normal \\\n",
    "    --emsize 300 \\\n",
    "    --nhid 300 \\\n",
    "    --use_clwe \\\n",
    "    --clwe_method RAND_TRANS \\\n",
    "    --panlex_loc ../../data/lexicon/eng-mic3.txt \\\n",
    "    --panlex_dist 1 \\\n",
    "    --clwe_save ../../models/embeddings/wiki.en \\\n",
    "    --save ../../models/rnns/init_models/gru_300_clwe_randtrans_dist1.pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jeremie/anaconda3/envs/pytorch_test/lib/python3.7/site-packages/torch/nn/modules/rnn.py:38: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n",
      "Starting initializing clwe\n",
      "Traceback (most recent call last):\n",
      "  File \"../../src/main.py\", line 201, in <module>\n",
      "    args.out_weights\n",
      "  File \"/home/jeremie/dev/HonoursCode/src/model.py\", line 94, in __init__\n",
      "    self.init_clwe_rand(cl_embeddings, vocab, panlex, concat=clwe_concat)\n",
      "  File \"/home/jeremie/dev/HonoursCode/src/model.py\", line 236, in init_clwe_rand\n",
      "    lexicon_list = list(lexicon.items())\n",
      "AttributeError: 'Panlex' object has no attribute 'items'\n"
     ]
    }
   ],
   "source": [
    "!command python ../../src/main.py \\\n",
    "    --data ../../data/transformed/micmac \\\n",
    "    --model GRU \\\n",
    "    --epochs=35 \\\n",
    "    --log-interval 25 \\\n",
    "    --tied \\\n",
    "    --nlayers 1 \\\n",
    "    --lr 5 \\\n",
    "    --dropout 0.5 \\\n",
    "    --hin_weights normal \\\n",
    "    --hr_weights normal \\\n",
    "    --em_weights normal \\\n",
    "    --out_weights normal \\\n",
    "    --emsize 300 \\\n",
    "    --nhid 300 \\\n",
    "    --use_clwe \\\n",
    "    --clwe_method RAND_COMBO \\\n",
    "    --panlex_loc ../../data/lexicon/eng-mic3.txt \\\n",
    "    --panlex_dist 1 \\\n",
    "    --clwe_save ../../models/embeddings/wiki.en \\\n",
    "    --save ../../models/rnns/init_models/gru_300_clwe_randcombo__dist1.pt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../../src/eval_model.py:78: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  probs = nn.Softmax()(output_flat[i])\n",
      "Perplexity: 673.0121521378508\n",
      "Adjusted Perplexity: 673.0121521378508\n",
      "../../src/eval_model.py:78: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  probs = nn.Softmax()(output_flat[i])\n",
      "Perplexity: 685.6543968541804\n",
      "Adjusted Perplexity: 685.6543968541804\n",
      "../../src/eval_model.py:78: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  probs = nn.Softmax()(output_flat[i])\n",
      "Perplexity: 743.3675104943843\n",
      "Adjusted Perplexity: 743.3675104943843\n"
     ]
    }
   ],
   "source": [
    "!command python ../../src/eval_model.py \\\n",
    "    --model ../../models/rnns/init_models/gru_300_clwe.pt \\\n",
    "    --data ../../data/transformed/micmac/ \\\n",
    "    > ../../data/eval_out/rnns/init_models/gru_300_clwe.txt\n",
    "\n",
    "!command python ../../src/eval/perplexity_adj.py \\\n",
    "        ../../data/eval_out/rnns/init_models/gru_300_clwe.txt\n",
    "\n",
    "!command python ../../src/eval_model.py \\\n",
    "    --model ../../models/rnns/init_models/gru_200_clwe.pt \\\n",
    "    --data ../../data/transformed/micmac/ \\\n",
    "    > ../../data/eval_out/rnns/init_models/gru_200_clwe.txt\n",
    "\n",
    "!command python ../../src/eval/perplexity_adj.py \\\n",
    "        ../../data/eval_out/rnns/init_models/gru_200_clwe.txt\n",
    "\n",
    "!command python ../../src/eval_model.py \\\n",
    "    --model ../../models/rnns/init_models/LSTM_300_clwe.pt \\\n",
    "    --data ../../data/transformed/micmac/ \\\n",
    "    > ../../data/eval_out/rnns/init_models/LSTM_300_clwe.txt\n",
    "\n",
    "!command python ../../src/eval/perplexity_adj.py \\\n",
    "        ../../data/eval_out/rnns/init_models/LSTM_300_clwe.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../../src/eval_model.py:78: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  probs = nn.Softmax()(output_flat[i])\n",
      "Perplexity: 741.646257671602\n",
      "Adjusted Perplexity: 741.646257671602\n",
      "../../src/eval_model.py:78: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  probs = nn.Softmax()(output_flat[i])\n",
      "Perplexity: 705.2504105475073\n",
      "Adjusted Perplexity: 705.2504105475073\n",
      "../../src/eval_model.py:78: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  probs = nn.Softmax()(output_flat[i])\n",
      "Perplexity: 699.070038387091\n",
      "Adjusted Perplexity: 699.070038387091\n"
     ]
    }
   ],
   "source": [
    "!command python ../../src/eval_model.py \\\n",
    "    --model ../../models/rnns/init_models/gru_300_clwe_RAND.pt \\\n",
    "    --data ../../data/transformed/micmac/ \\\n",
    "    > ../../data/eval_out/rnns/init_models/gru_300_clwe_RAND.txt\n",
    "\n",
    "!command python ../../src/eval/perplexity_adj.py \\\n",
    "        ../../data/eval_out/rnns/init_models/gru_300_clwe_RAND.txt\n",
    "\n",
    "!command python ../../src/eval_model.py \\\n",
    "    --model ../../models/rnns/init_models/gru_200_clwe_RAND.pt \\\n",
    "    --data ../../data/transformed/micmac/ \\\n",
    "    > ../../data/eval_out/rnns/init_models/gru_200_clwe_RAND.txt\n",
    "\n",
    "!command python ../../src/eval/perplexity_adj.py \\\n",
    "        ../../data/eval_out/rnns/init_models/gru_200_clwe_RAND.txt\n",
    "\n",
    "!command python ../../src/eval_model.py \\\n",
    "    --model ../../models/rnns/init_models/LSTM_300_clwe_RAND.pt \\\n",
    "    --data ../../data/transformed/micmac/ \\\n",
    "    > ../../data/eval_out/rnns/init_models/LSTM_300_clwe_RAND.txt\n",
    "\n",
    "!command python ../../src/eval/perplexity_adj.py \\\n",
    "        ../../data/eval_out/rnns/init_models/LSTM_300_clwe_RAND.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../../src/eval_model.py:78: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  probs = nn.Softmax()(output_flat[i])\n",
      "Perplexity: 683.777479086394\n",
      "Adjusted Perplexity: 683.777479086394\n",
      "../../src/eval_model.py:78: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  probs = nn.Softmax()(output_flat[i])\n",
      "Perplexity: 688.4606932799184\n",
      "Adjusted Perplexity: 688.4606932799184\n",
      "../../src/eval_model.py:78: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  probs = nn.Softmax()(output_flat[i])\n",
      "Perplexity: 718.8919841744892\n",
      "Adjusted Perplexity: 718.8919841744892\n"
     ]
    }
   ],
   "source": [
    "!command python ../../src/eval_model.py \\\n",
    "    --model ../../models/rnns/init_models/gru_300_clwe_RAND_TRANS.pt \\\n",
    "    --data ../../data/transformed/micmac/ \\\n",
    "    > ../../data/eval_out/rnns/init_models/gru_300_clwe_RAND_TRANS.txt\n",
    "\n",
    "!command python ../../src/eval/perplexity_adj.py \\\n",
    "        ../../data/eval_out/rnns/init_models/gru_300_clwe_RAND_TRANS.txt\n",
    "\n",
    "!command python ../../src/eval_model.py \\\n",
    "    --model ../../models/rnns/init_models/gru_200_clwe_RAND_TRANS.pt \\\n",
    "    --data ../../data/transformed/micmac/ \\\n",
    "    > ../../data/eval_out/rnns/init_models/gru_200_clwe_RAND_TRANS.txt\n",
    "\n",
    "!command python ../../src/eval/perplexity_adj.py \\\n",
    "        ../../data/eval_out/rnns/init_models/gru_200_clwe_RAND_TRANS.txt\n",
    "\n",
    "!command python ../../src/eval_model.py \\\n",
    "    --model ../../models/rnns/init_models/LSTM_300_clwe_RAND_TRANS.pt \\\n",
    "    --data ../../data/transformed/micmac/ \\\n",
    "    > ../../data/eval_out/rnns/init_models/LSTM_300_clwe_RAND_TRANS.txt\n",
    "\n",
    "!command python ../../src/eval/perplexity_adj.py \\\n",
    "        ../../data/eval_out/rnns/init_models/LSTM_300_clwe_RAND_TRANS.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../../src/eval_model.py:78: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  probs = nn.Softmax()(output_flat[i])\n",
      "Perplexity: 673.0121521378508\n",
      "Adjusted Perplexity: 673.0121521378508\n",
      "../../src/eval_model.py:78: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  probs = nn.Softmax()(output_flat[i])\n",
      "Perplexity: 685.6543968541804\n",
      "Adjusted Perplexity: 685.6543968541804\n",
      "../../src/eval_model.py:78: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  probs = nn.Softmax()(output_flat[i])\n",
      "Perplexity: 743.3675104943843\n",
      "Adjusted Perplexity: 743.3675104943843\n"
     ]
    }
   ],
   "source": [
    "!command python ../../src/eval_model.py \\\n",
    "    --model ../../models/rnns/init_models/gru_300_clwe_RAND_COMBO.pt \\\n",
    "    --data ../../data/transformed/micmac/ \\\n",
    "    > ../../data/eval_out/rnns/init_models/gru_300_clwe_RAND_COMBO.txt\n",
    "\n",
    "!command python ../../src/eval/perplexity_adj.py \\\n",
    "        ../../data/eval_out/rnns/init_models/gru_300_clwe_RAND_COMBO.txt\n",
    "\n",
    "!command python ../../src/eval_model.py \\\n",
    "    --model ../../models/rnns/init_models/gru_200_clwe_RAND_COMBO.pt \\\n",
    "    --data ../../data/transformed/micmac/ \\\n",
    "    > ../../data/eval_out/rnns/init_models/gru_200_clwe_RAND_COMBO.txt\n",
    "\n",
    "!command python ../../src/eval/perplexity_adj.py \\\n",
    "        ../../data/eval_out/rnns/init_models/gru_200_clwe_RAND_COMBO.txt\n",
    "\n",
    "!command python ../../src/eval_model.py \\\n",
    "    --model ../../models/rnns/init_models/LSTM_300_clwe_RAND_COMBO.pt \\\n",
    "    --data ../../data/transformed/micmac/ \\\n",
    "    > ../../data/eval_out/rnns/init_models/LSTM_300_clwe_RAND_COMBO.txt\n",
    "\n",
    "!command python ../../src/eval/perplexity_adj.py \\\n",
    "        ../../data/eval_out/rnns/init_models/LSTM_300_clwe_RAND_COMBO.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../../src/eval_model.py:78: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  probs = nn.Softmax()(output_flat[i])\n",
      "Perplexity: 1244.4258092028442\n",
      "Adjusted Perplexity: 1244.4258092028442\n",
      "../../src/eval_model.py:78: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  probs = nn.Softmax()(output_flat[i])\n",
      "Perplexity: 980.2519051975551\n",
      "Adjusted Perplexity: 980.2519051975551\n"
     ]
    }
   ],
   "source": [
    "!command python ../../src/eval_model.py \\\n",
    "    --model ../../models/rnns/init_models/gru_300_clwe_DUONG_5m.pt \\\n",
    "    --data ../../data/transformed/micmac/ \\\n",
    "    > ../../data/eval_out/rnns/init_models/gru_300_clwe_DUONG_5m.txt\n",
    "\n",
    "!command python ../../src/eval/perplexity_adj.py \\\n",
    "        ../../data/eval_out/rnns/init_models/gru_300_clwe_DUONG_5m.txt\n",
    "\n",
    "!command python ../../src/eval_model.py \\\n",
    "    --model ../../models/rnns/init_models/gru_300_clwe_DUONG_200k.pt \\\n",
    "    --data ../../data/transformed/micmac/ \\\n",
    "    > ../../data/eval_out/rnns/init_models/gru_300_clwe_DUONG_200k.txt\n",
    "\n",
    "!command python ../../src/eval/perplexity_adj.py \\\n",
    "        ../../data/eval_out/rnns/init_models/gru_300_clwe_DUONG_200k.txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../../src/eval_model.py:78: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  probs = nn.Softmax()(output_flat[i])\n",
      "Perplexity: 1050.4024640498124\n",
      "Adjusted Perplexity: 1050.4024640498124\n",
      "Traceback (most recent call last):\n",
      "  File \"../../src/eval_model.py\", line 25, in <module>\n",
      "    with open(args.model, 'rb') as f:\n",
      "FileNotFoundError: [Errno 2] No such file or directory: '../../models/rnns/init_models/gru_200_clwe_DUONG_200k.pt'\n",
      "Traceback (most recent call last):\n",
      "  File \"../../src/eval/perplexity_adj.py\", line 46, in <module>\n",
      "    perplexity_calc(open(sys.argv[1]))\n",
      "  File \"../../src/eval/perplexity_adj.py\", line 31, in perplexity_calc\n",
      "    perplexity = (-1/num_tokens) * lp_sum\n",
      "ZeroDivisionError: division by zero\n"
     ]
    }
   ],
   "source": [
    "!command python ../../src/eval_model.py \\\n",
    "    --model ../../models/rnns/init_models/gru_200_clwe_DUONG_5m.pt \\\n",
    "    --data ../../data/transformed/micmac/ \\\n",
    "    > ../../data/eval_out/rnns/init_models/gru_200_clwe_DUONG_5m.txt\n",
    "\n",
    "!command python ../../src/eval/perplexity_adj.py \\\n",
    "        ../../data/eval_out/rnns/init_models/gru_200_clwe_DUONG_5m.txt\n",
    "\n",
    "!command python ../../src/eval_model.py \\\n",
    "    --model ../../models/rnns/init_models/gru_200_clwe_DUONG_200k.pt \\\n",
    "    --data ../../data/transformed/micmac/ \\\n",
    "    > ../../data/eval_out/rnns/init_models/gru_200_clwe_DUONG_200k.txt\n",
    "\n",
    "!command python ../../src/eval/perplexity_adj.py \\\n",
    "        ../../data/eval_out/rnns/init_models/gru_200_clwe_DUONG_200k.txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../../src/eval_model.py:78: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  probs = nn.Softmax()(output_flat[i])\n",
      "Perplexity: 933.0281630969756\n",
      "Adjusted Perplexity: 933.0281630969756\n",
      "../../src/eval_model.py:78: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  probs = nn.Softmax()(output_flat[i])\n",
      "Perplexity: 889.2140069734409\n",
      "Adjusted Perplexity: 889.2140069734409\n"
     ]
    }
   ],
   "source": [
    "!command python ../../src/eval_model.py \\\n",
    "    --model ../../models/rnns/init_models/LSTM_300_clwe_DUONG_5m.pt \\\n",
    "    --data ../../data/transformed/micmac/ \\\n",
    "    > ../../data/eval_out/rnns/init_models/LSTM_300_clwe_DUONG_5m.txt\n",
    "\n",
    "!command python ../../src/eval/perplexity_adj.py \\\n",
    "        ../../data/eval_out/rnns/init_models/LSTM_300_clwe_DUONG_5m.txt\n",
    "\n",
    "!command python ../../src/eval_model.py \\\n",
    "    --model ../../models/rnns/init_models/LSTM_300_clwe_DUONG_200k.pt \\\n",
    "    --data ../../data/transformed/micmac/ \\\n",
    "    > ../../data/eval_out/rnns/init_models/LSTM_300_clwe_DUONG_200k.txt\n",
    "\n",
    "!command python ../../src/eval/perplexity_adj.py \\\n",
    "        ../../data/eval_out/rnns/init_models/LSTM_300_clwe_DUONG_200k.txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../../src/eval_model.py:78: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  probs = nn.Softmax()(output_flat[i])\n",
      "Perplexity: 1166.3660296390515\n",
      "Adjusted Perplexity: 1166.3660296390515\n",
      "../../src/eval_model.py:78: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  probs = nn.Softmax()(output_flat[i])\n",
      "Perplexity: 1214.4515515659334\n",
      "Adjusted Perplexity: 1214.4515515659334\n",
      "../../src/eval_model.py:78: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  probs = nn.Softmax()(output_flat[i])\n",
      "Perplexity: 1101.9576338365493\n",
      "Adjusted Perplexity: 1101.9576338365493\n",
      "../../src/eval_model.py:78: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  probs = nn.Softmax()(output_flat[i])\n",
      "Perplexity: 1147.4851127598415\n",
      "Adjusted Perplexity: 1147.4851127598415\n"
     ]
    }
   ],
   "source": [
    "\n",
    "!command python ../../src/eval_model.py \\\n",
    "    --model ../../models/rnns/init_models/gru_300_clwe_DUONG_200k_f.pt \\\n",
    "    --data ../../data/transformed/micmac/ \\\n",
    "    > ../../data/eval_out/rnns/init_models/gru_300_clwe_DUONG_200k_f.txt\n",
    "\n",
    "!command python ../../src/eval/perplexity_adj.py \\\n",
    "        ../../data/eval_out/rnns/init_models/gru_300_clwe_DUONG_200k_f.txt\n",
    "\n",
    "!command python ../../src/eval_model.py \\\n",
    "    --model ../../models/rnns/init_models/gru_300_clwe_DUONG_200k_fskip.pt \\\n",
    "    --data ../../data/transformed/micmac/ \\\n",
    "    > ../../data/eval_out/rnns/init_models/gru_300_clwe_DUONG_200k_fskip.txt\n",
    "\n",
    "!command python ../../src/eval/perplexity_adj.py \\\n",
    "        ../../data/eval_out/rnns/init_models/gru_300_clwe_DUONG_200k_fskip.txt\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "!command python ../../src/eval_model.py \\\n",
    "    --model ../../models/rnns/init_models/lstm_300_clwe_DUONG_200k_f.pt \\\n",
    "    --data ../../data/transformed/micmac/ \\\n",
    "    > ../../data/eval_out/rnns/init_models/lstm_300_clwe_DUONG_200k_f.txt\n",
    "\n",
    "!command python ../../src/eval/perplexity_adj.py \\\n",
    "        ../../data/eval_out/rnns/init_models/lstm_300_clwe_DUONG_200k_f.txt\n",
    "\n",
    "!command python ../../src/eval_model.py \\\n",
    "    --model ../../models/rnns/init_models/lstm_300_clwe_DUONG_200k_fskip.pt \\\n",
    "    --data ../../data/transformed/micmac/ \\\n",
    "    > ../../data/eval_out/rnns/init_models/lstm_300_clwe_DUONG_200k_fskip.txt\n",
    "\n",
    "!command python ../../src/eval/perplexity_adj.py \\\n",
    "        ../../data/eval_out/rnns/init_models/lstm_300_clwe_DUONG_200k_fskip.txt\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
